{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "\n",
    "from torchvision.datasets import SVHN\n",
    "from torchvision import transforms as tr\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "from inpainting.custom_layers import Reshape\n",
    "from inpainting.losses import nll_masked_batch_loss, nll_masked_batch_loss_components, loss_with_mixup, nll_masked_batch_loss_same_size_masks\n",
    "from pathlib import Path\n",
    "import inpainting.visualizations.samples as vis\n",
    "from inpainting.visualizations.digits import rgb_with_mask\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inpainting.datasets.celeba import train_val_datasets as celeba_train_val_ds, DEFAULT_MASK_CONFIGS\n",
    "from inpainting.datasets.svhn import train_val_datasets as svhn_train_val_ds\n",
    "from inpainting.datasets.mask_coding import UNKNOWN_LOSS\n",
    "from inpainting.datasets.utils import RandomRectangleMaskConfig\n",
    "from inpainting.visualizations.digits import rgb_with_mask \n",
    "from inpainting.training import train_inpainter\n",
    "from inpainting.utils import classifier_experiment, inpainted, predictions_for_entire_loader\n",
    "from inpainting.inpainters.rgb import RGBInpainter\n",
    "from inpainting.inpainters.fullconv import FullyConvolutionalInpainter\n",
    "from inpainting import backbones as bkb\n",
    "from inpainting import losses2 as l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = \"white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ps aux | grep mprzewie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = Path(\"../results/celeba/fullconv/64x64_64_3_64_no_latent_m_off_0.5\")\n",
    "experiment_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_val = celeba_train_val_ds(\n",
    "    Path.home() / \"uj/.data/\", \n",
    "    mask_configs=[RandomRectangleMaskConfig(UNKNOWN_LOSS,32, 32)],\n",
    "    resize_size=(120, 120),\n",
    "    crop_size=(64, 64),\n",
    "    deterministic=False\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(15, 15))\n",
    "for i in range(100):\n",
    "    (x,j), y = ds_train[i]\n",
    "#     print(x.shape)\n",
    "    ax = axes[i // 10, i%10]\n",
    "#     ax.set_title(f\"{y}\")\n",
    "    rgb_with_mask(x.numpy(), j.numpy(), ax)\n",
    "train_fig = plt.gcf()\n",
    "train_fig.savefig(experiment_path / \"train.png\")\n",
    "plt.show()\n",
    "\n",
    "len(ds_train), len(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=24\n",
    "dl_train = DataLoader(ds_train, batch_size, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_noms = lambda x,j,p,m,a,d : nll_masked_batch_loss_components(\n",
    "    x,j,p,m,a,d\n",
    ")[\"log_noms\"]\n",
    "\n",
    "x_minus_means = lambda x,j,p,m,a,d : nll_masked_batch_loss_components(\n",
    "    x,j,p,m,a,d\n",
    ")[\"x_minus_means\"]\n",
    "\n",
    "x_minus_means_2 = lambda x,j,p,m,a,d : nll_masked_batch_loss_components(\n",
    "    x,j,p,m,a,d\n",
    ")[\"x_minus_means_2\"]\n",
    "\n",
    "log_dets = lambda x,j,p,m,a,d : nll_masked_batch_loss_components(\n",
    "    x,j,p,m,a,d\n",
    ")[\"log_dets\"]\n",
    "\n",
    "log_2_pi = lambda x,j,p,m,a,d : nll_masked_batch_loss_components(\n",
    "    x,j,p,m,a,d\n",
    ")[\"log_2_pi\"]\n",
    "\n",
    "a_var = lambda x,j,p,m,a,d: a.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64x64 - 2416090845\n",
    "\n",
    "32x32 - 151092957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "inpainter = FullyConvolutionalInpainter(\n",
    "    n_mixes=1, a_width=4, a_amplitude=1, c_h_w=(3, 64, 64), last_channels=64, m_offset=0.5,\n",
    "\n",
    "    extractor=bkb.down_up_backbone(\n",
    "        (6, 64, 64),\n",
    "        depth=3,\n",
    "        first_channels=64, \n",
    "        last_channels=64,\n",
    "        kernel_size=5,\n",
    "        latent=False\n",
    "    ),\n",
    ")\n",
    "# inpainter.to(device)\n",
    "opt = optim.Adam(inpainter.parameters(), lr=4e-5, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_path = experiment_path / \"inpainter.state\"\n",
    "if ckp_path.exists():\n",
    "    print(\"resuming model\")\n",
    "    chckp = torch.load(ckp_path)\n",
    "    inpainter.load_state_dict(chckp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainter.train()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "history = train_inpainter(\n",
    "    inpainter, \n",
    "    dl_train, \n",
    "    dl_val, \n",
    "    opt, \n",
    "    loss_fn =l2.nll_buffered, \n",
    "    n_epochs=n_epochs,\n",
    "    device=device,\n",
    "    max_benchmark_batches=200,\n",
    "    \n",
    "    losses_to_log=dict(\n",
    "        mse=l2.loss_factory(\n",
    "            gathering_fn=l2.buffered_gather_batch_by_mask_indices,\n",
    "            calc_fn=l2.mse\n",
    "        )\n",
    "    ),\n",
    "    tqdm_loader=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (experiment_path / \"history_last_epoch.pkl\").open(\"rb\") as f:\n",
    "#     history = [pickle.load(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with (experiment_path / \"history_last_epoch.pkl\").open(\"wb\") as f:\n",
    "    history_tmp = history\n",
    "    pickle.dump(history[-1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (experiment_path / \"inpainter.schema\").open(\"w\") as f:\n",
    "    print(inpainter, file=f)\n",
    "with (experiment_path / \"opt.schema\").open(\"w\") as f:\n",
    "    print(opt, file=f)\n",
    "\n",
    "# torch.save(inpainter.state_dict, experiment_path / \"inpainter.state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[h[\"losses\"] for h in history];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[-1][\"losses\"][\"objective\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tmp = history\n",
    "history = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_name in set(history[0][\"losses\"].keys()):\n",
    "    for fold in [\"train\", \"val\"]:\n",
    "        \n",
    "        plt.plot(\n",
    "            list(range(len(history))),\n",
    "            [h[\"losses\"][loss_name][fold] for h in history],\n",
    "            label=fold\n",
    "        )\n",
    "    plt.title(loss_name)\n",
    "    plt.legend()\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(experiment_path / f\"history.{loss_name}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 10\n",
    "\n",
    "row_length = vis.row_length(*list(zip(*history[0][\"sample_results\"][\"train\"]))[0])\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(history) / skip)* 2), \n",
    "    row_length,\n",
    "    figsize=(20, 30)\n",
    ")\n",
    "\n",
    "\n",
    "for e, h in enumerate(history):\n",
    "    if e % skip !=0 and e != (len(history) -1):\n",
    "        continue\n",
    "    \n",
    "    for ax_no, fold in [(0,\"train\"), (1,\"val\")]:\n",
    "        x, j, p, m, a, d, y = [t[0] for t in  h[\"sample_results\"][fold]]\n",
    "        row_no = (e // skip)*2 + ax_no\n",
    "\n",
    "        vis.visualize_sample(\n",
    "            x, j, p, m, a, d, y, \n",
    "            ax_row=axes[row_no], \n",
    "            title_prefixes={\n",
    "                0: f\"{e} {fold} \",\n",
    "#                 1: f\"y_m = {y_masked_pred}\"\n",
    "            },\n",
    "            drawing_fn=rgb_with_mask\n",
    "        )\n",
    "\n",
    "epochs_fig = plt.gcf()\n",
    "epochs_fig.savefig(experiment_path / \"epochs_renders.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_path = experiment_path / \"epochs\"\n",
    "epochs_path.mkdir(exist_ok=True)\n",
    "skip = 10\n",
    "n_rows = 16\n",
    "\n",
    "for e, h in enumerate(history):\n",
    "    if e % skip !=0 and e != (len(history) -1):\n",
    "        continue\n",
    "    \n",
    "    for ax_no, fold in [(0,\"train\"), (1,\"val\")]:\n",
    "        \n",
    "        \n",
    "        row_length = vis.row_length(*list(zip(*h[\"sample_results\"][fold]))[0])\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            n_rows, \n",
    "            row_length,\n",
    "            figsize=(20, 30)\n",
    "        )\n",
    "\n",
    "        for row_no, (x, j, p, m ,a, d, y) in enumerate(list(zip(*h[\"sample_results\"][fold]))[:n_rows]):\n",
    "            vis.visualize_sample(\n",
    "                x, j, p, m, a, d, y, \n",
    "                ax_row=axes[row_no], \n",
    "                title_prefixes={\n",
    "                    0: f\"{e} {fold} \",\n",
    "#                     1: f\"y_m = {y_masked_pred}\"\n",
    "                },\n",
    "                drawing_fn=rgb_with_mask\n",
    "            )\n",
    "        \n",
    "        title = f\"{e}_{fold}\"\n",
    "        plt.suptitle(title)\n",
    "        plt.savefig(epochs_path / f\"{title}.png\")\n",
    "#         plt.show()\n",
    "            \n",
    "\n",
    "# epochs_fig = plt.gcf()\n",
    "# epochs_fig.savefig(experiment_path / \"epochs_renders.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_arrays_stats(\n",
    "    arrays, ax=None, stat_fns = [np.min, np.max, np.mean], markers=\".\"\n",
    "):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "    if isinstance(markers, str):\n",
    "        markers = [markers] * len(stat_fns)\n",
    "    \n",
    "    for fn, m in zip(stat_fns, markers):\n",
    "        ax.scatter(\n",
    "            range(len(arrays)),\n",
    "            [\n",
    "                fn(a) for a in arrays\n",
    "            ],\n",
    "            marker=m,\n",
    "            label=fn.__name__\n",
    "            \n",
    "        )\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_last_epoch = history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in [\"val\"]:\n",
    "    x, j, p, m, a, d, y = hist_last_epoch[\"sample_results\"][fold]\n",
    "\n",
    "    a_resh = a.reshape(a.shape[0] * a.shape[1], a.shape[2], a.shape[3])\n",
    "    covs = a_resh.transpose(0, 2, 1) @ a_resh\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 25), nrows=5)\n",
    "    \n",
    "    ax[0].set_title(f\"m stats {fold}\")\n",
    "    plot_arrays_stats(\n",
    "        m, ax[0]\n",
    "    )\n",
    "        \n",
    "    samples = [\n",
    "        vis.gans_gmms_sample_no_d(x_, m_[0], a_[0], d_[0])\n",
    "        for (x_, m_, a_, d_) in zip(x, m ,a, d)\n",
    "    ]\n",
    "    \n",
    "    ax[1].set_title(\"samples stats\")\n",
    "    plot_arrays_stats(samples, ax[1])\n",
    "    \n",
    "    ax[2].set_title(f\"a stats {fold}\")\n",
    "    plot_arrays_stats(\n",
    "        a, ax[2]\n",
    "    )\n",
    "    \n",
    "    ax[3].set_title(f\"d stats {fold}\")\n",
    "    plot_arrays_stats(\n",
    "        d, ax[3]\n",
    "    )\n",
    "    \n",
    "    ax[4].set_title(f\"cov stats {fold}\")\n",
    "    plot_arrays_stats(\n",
    "        covs, ax[4]\n",
    "    )\n",
    "    [a.legend() for a in ax[:5]]\n",
    "    fig.savefig(experiment_path / f\"outputs_stats.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    cov_resh = covs[0].reshape(-1)\n",
    "    plt.hist(cov_resh, log=True, bins=100)\n",
    "    plt.title(f\"cov[0] hist {fold}\")\n",
    "    plt.show()\n",
    "    \n",
    "    cov = covs[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), nrows=2)\n",
    "    eigs = np.linalg.eigvals(cov)\n",
    "    ax[0].scatter(range(len(eigs)), eigs)\n",
    "    ax[0].set_title(\"eigenvals of cov[0]\")\n",
    "\n",
    "    cov_d = cov + np.diag(d[0])\n",
    "    eigs_d = np.linalg.eigvals(cov_d)\n",
    "    ax[1].scatter(range(len(eigs_d)), eigs_d)\n",
    "    ax[1].set_title(\"eigenvals of cov[0] + d[0]\")\n",
    "    fig.savefig(experiment_path / \"eigenvals.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # wygląda na to, że mamy ~3 duże wartosci własne\n",
    "    \n",
    "    print(\"m analysis\")\n",
    "    \n",
    "    \n",
    "    plt.hist(d[0].reshape(-1), bins=100, log=True)\n",
    "    plt.title(\"d[0] hist\")\n",
    "    plt.show()\n",
    "    \n",
    "#     for i in range(3):\n",
    "#         plt.imshow(a_resh[0, i].reshape(28,28), cmap=\"gray\")\n",
    "#         plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = predictions_for_entire_loader(\n",
    "    inpainter.to(torch.device(\"cpu\")),\n",
    "    dl_val,\n",
    "    torch.device(\"cpu\")\n",
    "    \n",
    ")\n",
    "with (experiment_path / \"val_predictions.pkl\").open(\"wb\") as f:\n",
    "    pickle.dump(val_results, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 11, figsize=(15,15)\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    vis.visualize_sample(\n",
    "        *val_results[i],\n",
    "        drawing_fn=rgb_with_mask,\n",
    "        title_prefixes=dict(),\n",
    "        ax_row=ax[i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
