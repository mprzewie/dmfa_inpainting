{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torchvision import transforms as tr\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "from typing import Tuple\n",
    "from inpainting.custom_layers import Reshape\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisInpainter(\n",
    "    nn.Module\n",
    "):\n",
    "    def __init__(self, n_mixes: int = 1, in_size: int = 4, a_width = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(in_size * 2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.a_extractor = nn.Sequential(\n",
    "            nn.Linear(20, in_size * n_mixes * a_width),\n",
    "            Reshape((-1, n_mixes, a_width, in_size,)) # * L, we don't want 1x4 vector but L x4 matrix))\n",
    "        )\n",
    "        self.m_extractor = nn.Sequential(\n",
    "            nn.Linear(20, n_mixes * in_size),\n",
    "            Reshape((-1, n_mixes, in_size))\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.d_extractor = nn.Sequential(\n",
    "            nn.Linear(20, n_mixes * in_size),\n",
    "            Reshape((-1, n_mixes, in_size))\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.p_extractor = nn.Sequential(\n",
    "            nn.Linear(20, n_mixes),\n",
    "            nn.Softmax()\n",
    "        ) # omit this, let's say p = 1 for now\n",
    "\n",
    "    def forward(self, X, J):\n",
    "        X_masked = X * J\n",
    "        X_J = torch.cat([X_masked, J], dim=1)\n",
    "        features = self.extractor(X_J)\n",
    "        m = self.m_extractor(features)\n",
    "        d = self.d_extractor(features)\n",
    "        p = self.p_extractor(features)\n",
    "        a = self.a_extractor(features)\n",
    "        \n",
    "        return  p, m, a, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nll_loss(X, J, P, M, A, D) -> torch.autograd.Variable:\n",
    "    zipped = zip(X, J, P, M, A, D)\n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    for i, (x, j, p, m, a, d) in enumerate(zipped):\n",
    "        mask_inds = (j==0).nonzero().squeeze()\n",
    "        x_masked = torch.index_select(x, 0, mask_inds)\n",
    "        a_masked = torch.index_select(a, 2, mask_inds)\n",
    "        m_masked, d_masked = [\n",
    "            torch.index_select(t, 1, mask_inds)\n",
    "            for t in [m, d]\n",
    "        ]\n",
    "        \n",
    "        for (p_i, m_i, d_i, a_i) in zip(p, m_masked, d_masked, a_masked):\n",
    "            if a_i.shape[1] > 0:\n",
    "                cov = (a_i.T @ a_i) + torch.diag(d_i ** 2)\n",
    "                mvn_d = dist.MultivariateNormal(m_i, cov) # calculate this manually\n",
    "                l = - mvn_d.log_prob(x_masked) \n",
    "                losses.append(l)\n",
    "            else:\n",
    "                losses.append(torch.autograd.Variable(torch.tensor(0.0), requires_grad=True))\n",
    "    return torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(thresh: float) -> Tuple[float, float, float]:\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris['data']\n",
    "    y = iris['target']\n",
    "    np.random.seed(0)\n",
    "    J = np.random.uniform(size=X.shape)\n",
    "    J[J < thresh] = 0\n",
    "    J[J >= thresh] = 1\n",
    "    X_train, X_test, J_train, J_test, y_train, y_test = train_test_split(X, J, y, test_size=0.33, random_state=0)\n",
    "    ds_train = TensorDataset(\n",
    "        torch.Tensor(X_train), \n",
    "        torch.Tensor(J_train),\n",
    "        torch.Tensor(y_train).long()\n",
    "    )\n",
    "\n",
    "    ds_test = TensorDataset(\n",
    "        torch.Tensor(X_test), \n",
    "        torch.Tensor(J_test),\n",
    "        torch.Tensor(y_test).long()\n",
    "    )\n",
    "\n",
    "    batch_size=16\n",
    "    dl_train = DataLoader(ds_train, batch_size, shuffle=True)\n",
    "    dl_test = DataLoader(ds_test, batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    inpainter = IrisInpainter()\n",
    "    opt = optim.Adam(inpainter.parameters(), lr=4e-3)\n",
    "    n_epochs = 100\n",
    "    train_hist = []\n",
    "    val_hist = []\n",
    "    for e in range(n_epochs):\n",
    "        inpainter.train()\n",
    "        train_losses = [] \n",
    "        for i, (x, j, y) in enumerate(dl_train):\n",
    "            inpainter.zero_grad()\n",
    "            p, m, a, d = inpainter(x, j)\n",
    "            loss = nll_loss(x, j, p, m, a, d)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_losses.append(loss)\n",
    "        train_hist.append(torch.stack(train_losses).mean())\n",
    "\n",
    "        inpainter.eval()\n",
    "        val_losses = []\n",
    "        for i, (x, j, y) in enumerate(dl_test):\n",
    "            inpainter.zero_grad()\n",
    "            p, m, a, d = inpainter(x, j)\n",
    "            loss = nll_loss(x, j, p, m, a, d)\n",
    "            val_losses.append(loss)\n",
    "        val_hist.append(torch.stack(val_losses).mean())\n",
    "\n",
    "    #     print(train_hist[-1], val_losses[-1])\n",
    "    \n",
    "    \n",
    "    full_classifier = MLPClassifier((10, 20, 10, 4), learning_rate_init=4e-3, max_iter=1000, random_state=0 ).fit(X_train, y_train)\n",
    "    full_full_acc = accuracy_score(full_classifier.predict(X_test), y_test)\n",
    "    \n",
    "    X_test_masked = X_test * J_test\n",
    "    full_masked_acc = accuracy_score(full_classifier.predict(X_test_masked), y_test)\n",
    "    \n",
    "    X_train_masked = X_train * J_train\n",
    "    masked_classifier =  MLPClassifier((10, 20, 10, 4), learning_rate_init=4e-3, max_iter=1000, random_state=0 ).fit(\n",
    "        np.hstack([X_train_masked, J_train]), y_train\n",
    "    )\n",
    "    masked_full_acc = accuracy_score(masked_classifier.predict(np.hstack([X_test, np.ones_like(X_test)])), y_test)\n",
    "    masked_masked_acc = accuracy_score(masked_classifier.predict(np.hstack([X_test_masked, J_test])), y_test)\n",
    "    \n",
    "    \n",
    "    P_test, M_test, A_test, D_test = inpainter(torch.Tensor(X_test_masked), torch.Tensor(J_test))\n",
    "\n",
    "    X_test_inpainted = X_test_masked.copy()\n",
    "    X_test_inpainted[J_test == 0] = M_test.detach().numpy()[:, 0][J_test == 0]\n",
    "    full_inp_acc = accuracy_score(full_classifier.predict(X_test_inpainted), y_test)\n",
    "    masked_inp_acc = accuracy_score(masked_classifier.predict(np.hstack([X_test_inpainted, np.ones_like(X_test_inpainted)])), y_test)\n",
    "    return {\n",
    "        \"thresh\": thresh,\n",
    "        \"full_full\": full_full_acc,\n",
    "        \"full_masked\": full_masked_acc,\n",
    "        \"masked_full\": masked_full_acc,\n",
    "        \"masked_masked\": masked_masked_acc,\n",
    "        \"full_inp_acc\": full_inp_acc,\n",
    "        \"masked_inp_acc\": masked_inp_acc\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([\n",
    "    experiment(thresh)\n",
    "    for thresh in np.linspace(0.1, 1, 10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot(\"thresh\", [c for c in results.columns if c != \"thresh\"],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uj)",
   "language": "python",
   "name": "uj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
