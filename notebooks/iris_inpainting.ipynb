{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torchvision import transforms as tr\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "J = np.random.uniform(size=X.shape)\n",
    "thresh = 0.2\n",
    "J[J < thresh] = 0\n",
    "J[J >= thresh] = 1\n",
    "\n",
    "X.shape, J.shape, y.shape, set(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, J_train, J_test, y_train, y_test = train_test_split(X, J, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TensorDataset(\n",
    "    torch.Tensor(X_train), \n",
    "    torch.Tensor(J_train),\n",
    "    torch.Tensor(y_train).long()\n",
    ")\n",
    "\n",
    "ds_test = TensorDataset(\n",
    "    torch.Tensor(X_test), \n",
    "    torch.Tensor(J_test),\n",
    "    torch.Tensor(y_test).long()\n",
    ")\n",
    "\n",
    "batch_size=16\n",
    "dl_train = DataLoader(ds_train, batch_size, shuffle=True)\n",
    "dl_test = DataLoader(ds_test, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshaper(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_size = out_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(*self.out_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisInpainter(\n",
    "    nn.Module\n",
    "):\n",
    "    def __init__(self, n_mixes: int = 1, in_size: int = 4, a_width = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(in_size * 2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.a_extractor = nn.Sequential(\n",
    "            nn.Linear(20, in_size * n_mixes * a_width),\n",
    "            Reshaper((-1, n_mixes, a_width, in_size,)) # * L, we don't want 1x4 vector but L x4 matrix))\n",
    "        )\n",
    "        self.m_extractor = nn.Sequential(\n",
    "            nn.Linear(20, n_mixes * in_size),\n",
    "            Reshaper((-1, n_mixes, in_size))\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.d_extractor = nn.Sequential(\n",
    "            nn.Linear(20, n_mixes * in_size),\n",
    "            Reshaper((-1, n_mixes, in_size))\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.p_extractor = nn.Sequential(\n",
    "            nn.Linear(20, n_mixes),\n",
    "            nn.Softmax()\n",
    "        ) # omit this, let's say p = 1 for now\n",
    "\n",
    "    def forward(self, X, J):\n",
    "        X_masked = X * J\n",
    "        X_J = torch.cat([X_masked, J], dim=1)\n",
    "        features = self.extractor(X_J)\n",
    "        m = self.m_extractor(features)\n",
    "        d = self.d_extractor(features)\n",
    "        p = self.p_extractor(features)\n",
    "        a = self.a_extractor(features)\n",
    "        \n",
    "        return  p, m, a, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nll_loss(X, J, P, M, A, D) -> torch.autograd.Variable:\n",
    "    zipped = zip(X, J, P, M, A, D)\n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    for i, (x, j, p, m, a, d) in enumerate(zipped):\n",
    "        mask_inds = (j==0).nonzero().squeeze()\n",
    "        x_masked = torch.index_select(x, 0, mask_inds)\n",
    "        a_masked = torch.index_select(a, 2, mask_inds)\n",
    "        m_masked, d_masked = [\n",
    "            torch.index_select(t, 1, mask_inds)\n",
    "            for t in [m, d]\n",
    "        ]\n",
    "        \n",
    "        for (p_i, m_i, d_i, a_i) in zip(p, m_masked, d_masked, a_masked):\n",
    "            if a_i.shape[1] > 0:\n",
    "                cov = (a_i.transpose(0,1) @ a_i) + torch.diag(d_i ** 2)\n",
    "                mvn_d = dist.MultivariateNormal(m_i, cov) # calculate this manually\n",
    "                l = - mvn_d.log_prob(x_masked) \n",
    "                losses.append(l)\n",
    "            else:\n",
    "                losses.append(torch.tensor(0.0, requires_grad=True)) #.cuda())\n",
    "    return torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainter = IrisInpainter()\n",
    "# inpainter = inpainter.cuda()\n",
    "opt = optim.Adam(inpainter.parameters(), lr=4e-3)\n",
    "n_epochs = 100\n",
    "train_hist = []\n",
    "val_hist = []\n",
    "for e in tqdm(range(n_epochs)):\n",
    "    inpainter.train()\n",
    "    train_losses = [] \n",
    "    for i, (x, j, y) in enumerate(dl_train):\n",
    "        inpainter.zero_grad()\n",
    "#         x, j = x.cuda(), j.cuda()\n",
    "        p, m, a, d = inpainter(x, j)\n",
    "        loss = nll_loss(x, j, p, m, a, d)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_losses.append(loss)\n",
    "    train_hist.append(torch.stack(train_losses).mean())\n",
    "    \n",
    "    inpainter.eval()\n",
    "    val_losses = []\n",
    "    for i, (x, j, y) in enumerate(dl_test):\n",
    "        inpainter.zero_grad()\n",
    "#         x, j = x.cuda(), j.cuda()\n",
    "\n",
    "        p, m, a, d = inpainter(x, j)\n",
    "        loss = nll_loss(x, j, p, m, a, d)\n",
    "        val_losses.append(loss)\n",
    "    val_hist.append(torch.stack(val_losses).mean())\n",
    "\n",
    "#     print(train_hist[-1], val_losses[-1])\n",
    "\n",
    "plt.plot(list(range(n_epochs)), train_hist, label=\"train\")\n",
    "plt.plot(list(range(n_epochs)), val_hist, label=\"val\")\n",
    "plt.legend()\n",
    "print(val_losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MLPClassifier((10, 20, 10, 4), learning_rate_init=4e-3, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "accuracy_score(classifier.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_masked = X_test * J_test\n",
    "# X_test_masked[:, 0][J_test[:,0] == 0] = X_train[:,0].mean()\n",
    "# X_test_masked[:, 1][J_test[:,1] == 0] = X_train[:,1].mean()\n",
    "# X_test_masked[:, 2][J_test[:,2] == 0] = X_train[:,2].mean()\n",
    "# X_test_masked[:, 3][J_test[:,3] == 0] = X_train[:,3].mean()\n",
    "\n",
    "\n",
    "accuracy_score(classifier.predict(X_test_masked), y_test)\n",
    "# print(X_train.mean(axis=0))\n",
    "# X_test_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_test, M_test, A_test, D_test = inpainter(torch.Tensor(X_test_masked), torch.Tensor(J_test))\n",
    "\n",
    "X_test_inpainted = X_test_masked.copy()\n",
    "X_test_inpainted[J_test == 0] = M_test.detach().cpu().numpy()[:, 0][J_test == 0]\n",
    "\n",
    "# X_test_inpainted = M_test.detach().cpu().numpy()[:, 0]\n",
    "print(\"X_test[0]\", X_test[0]), \n",
    "print(\"X_test_masked[0]\", X_test_masked[0]) \n",
    "print(\"M_test.detach().numpy()[0, 0]\", M_test.detach().cpu().numpy()[0, 0]) \n",
    "print(\"X_test_inpainted[0]\", X_test_inpainted[0])\n",
    "\n",
    "accuracy_score(classifier.predict(X_test_inpainted), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_test, M_test, A_test, D_test = inpainter(torch.Tensor(X_test_masked), torch.Tensor(J_test))\n",
    "#\n",
    "nll_loss(torch.tensor(X_test).float(), torch.tensor(J_test).float(), P_test, M_test, A_test, D_test), \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss(torch.tensor(X_test).float(), torch.tensor(J_test).float(), P_test, torch.tensor(X_test).float().reshape(-1, 1, 4), A_test, D_test), \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uj)",
   "language": "python",
   "name": "uj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
